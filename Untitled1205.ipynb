{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92dfbcd-c901-4850-8383-b702b8ab1e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded elliptic_txs_features.csv enc='utf-8', sep=',', shape=(203768, 167)\n",
      "[INFO] Loaded elliptic_txs_classes.csv enc='utf-8', sep=',', shape=(203769, 2)\n",
      "[INFO] Loaded elliptic_txs_edgelist.csv enc='utf-8', sep=',', shape=(234355, 2)\n",
      "[INFO] Final dataset shape: (46564, 170)\n",
      "[INFO] Class balance:  {1: 42019, 0: 4545}\n",
      "[INFO] Split sizes: {'train': 26381, 'val': 8999, 'test': 11184}\n",
      "[INFO] Saved preprocessor.pkl\n",
      "[INFO] Class weights: {0: 4.594392197840474, 1: 0.5610591237771161}\n",
      "Epoch 1/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - accuracy: 0.6657 - auc: 0.8742 - loss: 0.4980 - pr_auc: 0.9783 - val_accuracy: 0.8322 - val_auc: 0.8554 - val_loss: 0.4862 - val_pr_auc: 0.9752\n",
      "Epoch 2/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8594 - auc: 0.9735 - loss: 0.2576 - pr_auc: 0.9959 - val_accuracy: 0.9195 - val_auc: 0.9017 - val_loss: 0.3064 - val_pr_auc: 0.9831\n",
      "Epoch 3/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9161 - auc: 0.9801 - loss: 0.1947 - pr_auc: 0.9966 - val_accuracy: 0.9293 - val_auc: 0.9282 - val_loss: 0.2217 - val_pr_auc: 0.9883\n",
      "Epoch 4/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9381 - auc: 0.9868 - loss: 0.1534 - pr_auc: 0.9980 - val_accuracy: 0.9418 - val_auc: 0.9483 - val_loss: 0.1804 - val_pr_auc: 0.9921\n",
      "Epoch 5/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9493 - auc: 0.9883 - loss: 0.1379 - pr_auc: 0.9983 - val_accuracy: 0.9315 - val_auc: 0.9550 - val_loss: 0.1887 - val_pr_auc: 0.9933\n",
      "Epoch 6/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9539 - auc: 0.9916 - loss: 0.1192 - pr_auc: 0.9989 - val_accuracy: 0.9384 - val_auc: 0.9538 - val_loss: 0.1668 - val_pr_auc: 0.9929\n",
      "Epoch 7/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9598 - auc: 0.9932 - loss: 0.1052 - pr_auc: 0.9991 - val_accuracy: 0.9270 - val_auc: 0.9577 - val_loss: 0.1804 - val_pr_auc: 0.9936\n",
      "Epoch 8/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9628 - auc: 0.9945 - loss: 0.0971 - pr_auc: 0.9993 - val_accuracy: 0.9350 - val_auc: 0.9617 - val_loss: 0.1632 - val_pr_auc: 0.9943\n",
      "Epoch 9/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9665 - auc: 0.9948 - loss: 0.0922 - pr_auc: 0.9993 - val_accuracy: 0.9370 - val_auc: 0.9558 - val_loss: 0.1616 - val_pr_auc: 0.9927\n",
      "Epoch 10/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9683 - auc: 0.9951 - loss: 0.0865 - pr_auc: 0.9994 - val_accuracy: 0.9301 - val_auc: 0.9498 - val_loss: 0.1807 - val_pr_auc: 0.9918\n",
      "Epoch 11/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9692 - auc: 0.9954 - loss: 0.0832 - pr_auc: 0.9994 - val_accuracy: 0.9255 - val_auc: 0.9522 - val_loss: 0.1888 - val_pr_auc: 0.9921\n",
      "Epoch 12/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9705 - auc: 0.9963 - loss: 0.0758 - pr_auc: 0.9995 - val_accuracy: 0.9297 - val_auc: 0.9556 - val_loss: 0.1802 - val_pr_auc: 0.9926\n",
      "Epoch 13/40\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9739 - auc: 0.9968 - loss: 0.0683 - pr_auc: 0.9996 - val_accuracy: 0.9261 - val_auc: 0.9501 - val_loss: 0.1922 - val_pr_auc: 0.9920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Best threshold on val: t=0.260 | F1=0.9710\n",
      "[INFO] Saved metrics.json and threshold.json\n",
      "{\n",
      "  \"n_train\": 26381,\n",
      "  \"n_val\": 8999,\n",
      "  \"n_test\": 11184,\n",
      "  \"roc_auc\": 0.8442063594278804,\n",
      "  \"pr_auc\": 0.9871204211354372,\n",
      "  \"brier\": 0.047455343374524,\n",
      "  \"acc_at_0.5\": 0.9441165951359084,\n",
      "  \"f1_at_0.5\": 0.970444980375467,\n",
      "  \"acc_at_best_t\": 0.9533261802575107,\n",
      "  \"f1_at_best_t\": 0.9755411863930278\n",
      "}\n",
      "[INFO] Saved model.h5 -> C:\\Users\\sagni\\Downloads\\GraphGuard\\model.h5\n",
      "[INFO] Saved model_config.yaml -> C:\\Users\\sagni\\Downloads\\GraphGuard\\model_config.yaml\n",
      "\n",
      "[DONE] Artifacts saved in: C:\\Users\\sagni\\Downloads\\GraphGuard\n",
      " - preprocessor.pkl\n",
      " - model.h5\n",
      " - model_config.yaml\n",
      " - metrics.json\n",
      " - threshold.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GraphGuard — Elliptic Bitcoin (TX graph) baseline trainer (dtype-safe)\n",
    "# Saves: preprocessor.pkl, model.h5, model_config.yaml, metrics.json, threshold.json\n",
    "# ============================================================\n",
    "import os, csv, json, math, pickle, warnings, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score, accuracy_score,\n",
    "    precision_recall_curve, roc_curve, brier_score_loss\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (YOUR EXACT FILES)\n",
    "# -----------------------------\n",
    "FEATURES_PATH = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_features.csv\"\n",
    "CLASSES_PATH  = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_classes.csv\"\n",
    "EDGES_PATH    = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_edgelist.csv\"\n",
    "OUTPUT_DIR    = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Robust CSV reader\n",
    "# -----------------------------\n",
    "def robust_read_csv(path, expected_min_cols=2):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"]\n",
    "    delims    = [\",\", \";\", \"\\t\", \"|\"]\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(8192).decode(\"latin1\", errors=\"ignore\")\n",
    "        sniffed = csv.Sniffer().sniff(head)\n",
    "        if sniffed.delimiter in delims:\n",
    "            delims = [sniffed.delimiter] + [d for d in delims if d != sniffed.delimiter]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        for sep in delims:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine=\"python\")\n",
    "                if df.shape[1] >= expected_min_cols:\n",
    "                    print(f\"[INFO] Loaded {os.path.basename(path)} enc='{enc}', sep='{sep}', shape={df.shape}\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "    raise RuntimeError(f\"Could not parse {path}. Last error: {last_err}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load data\n",
    "# -----------------------------\n",
    "df_feat = robust_read_csv(FEATURES_PATH, expected_min_cols=3)\n",
    "df_cls  = robust_read_csv(CLASSES_PATH,  expected_min_cols=2)\n",
    "df_edge = robust_read_csv(EDGES_PATH,    expected_min_cols=2)\n",
    "\n",
    "# Canonicalize column names (Elliptic uses txId, timeStep)\n",
    "feat_cols = list(df_feat.columns)\n",
    "if len(feat_cols) < 3:\n",
    "    raise RuntimeError(\"Features CSV must have >=3 columns (txId, timeStep, features...)\")\n",
    "\n",
    "tx_col_feat   = feat_cols[0]  # txId in features file\n",
    "time_col_feat = feat_cols[1]  # timeStep in features file\n",
    "feature_cols  = feat_cols[2:] # f1..fN\n",
    "\n",
    "# Classes file: typically [txId, class]\n",
    "cls_cols = list(df_cls.columns)\n",
    "tx_col_cls = cls_cols[0]\n",
    "class_col  = cls_cols[1]\n",
    "\n",
    "# Edge list: [src, dst] transaction graph (directed)\n",
    "edge_cols = list(df_edge.columns)\n",
    "src_col, dst_col = edge_cols[0], edge_cols[1]\n",
    "\n",
    "# -----------------------------\n",
    "# FORCE CONSISTENT DTYPE FOR TX IDs (fixes your error)\n",
    "# -----------------------------\n",
    "df_feat[tx_col_feat] = df_feat[tx_col_feat].astype(str)\n",
    "df_cls[tx_col_cls]   = df_cls[tx_col_cls].astype(str)\n",
    "df_edge[src_col]     = df_edge[src_col].astype(str)\n",
    "df_edge[dst_col]     = df_edge[dst_col].astype(str)\n",
    "\n",
    "# -----------------------------\n",
    "# Map labels: licit/illicit; drop unknown\n",
    "# Elliptic classes often: '1' = licit (0), '2' = illicit (1), 'unknown'\n",
    "# -----------------------------\n",
    "df_cls[class_col] = df_cls[class_col].astype(str).str.lower().str.strip()\n",
    "label_map = {\"1\": 0, \"2\": 1, \"licit\": 0, \"illicit\": 1}\n",
    "df_cls[\"label\"] = df_cls[class_col].map(label_map)\n",
    "df_cls = df_cls[~df_cls[\"label\"].isna()].copy()  # drop unknowns\n",
    "df_cls[\"label\"] = df_cls[\"label\"].astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# Compute (in/out) degrees from edgelist\n",
    "# -----------------------------\n",
    "in_deg  = df_edge.groupby(dst_col).size().rename(\"in_degree\")\n",
    "out_deg = df_edge.groupby(src_col).size().rename(\"out_degree\")\n",
    "deg_df  = pd.concat([in_deg, out_deg], axis=1).fillna(0.0).reset_index()\n",
    "deg_df.rename(columns={deg_df.columns[0]: tx_col_feat}, inplace=True)  # align merge key name\n",
    "\n",
    "# -----------------------------\n",
    "# Merge: features + degrees + labels\n",
    "# -----------------------------\n",
    "# time column should be numeric\n",
    "df_feat[time_col_feat] = pd.to_numeric(df_feat[time_col_feat], errors=\"coerce\")\n",
    "\n",
    "df = df_feat.merge(deg_df, on=tx_col_feat, how=\"left\")\n",
    "df[[\"in_degree\",\"out_degree\"]] = df[[\"in_degree\",\"out_degree\"]].fillna(0.0)\n",
    "\n",
    "df = df.merge(df_cls[[tx_col_cls,\"label\"]], left_on=tx_col_feat, right_on=tx_col_cls, how=\"inner\")\n",
    "# Drop duplicate right key\n",
    "if tx_col_cls in df.columns and tx_col_cls != tx_col_feat:\n",
    "    df = df.drop(columns=[tx_col_cls])\n",
    "\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"After merging features and labels, dataset is empty. Check txId column names and dtypes.\")\n",
    "\n",
    "# Final feature set: original f1..fN + degrees\n",
    "full_features = feature_cols + [\"in_degree\", \"out_degree\"]\n",
    "\n",
    "# Ensure numeric features\n",
    "for c in full_features:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=full_features + [time_col_feat, \"label\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"[INFO] Final dataset shape:\", df.shape)\n",
    "print(\"[INFO] Class balance: \", df[\"label\"].value_counts().to_dict())\n",
    "\n",
    "# -----------------------------\n",
    "# Time-based split (avoid leakage)\n",
    "#  - Split by unique timesteps: 60% train, 20% val, 20% test\n",
    "# -----------------------------\n",
    "steps = sorted(pd.unique(df[time_col_feat].values).tolist())\n",
    "n = len(steps)\n",
    "i1 = int(0.6*n); i2 = int(0.8*n)\n",
    "train_steps = set(steps[:i1])\n",
    "val_steps   = set(steps[i1:i2])\n",
    "test_steps  = set(steps[i2:])\n",
    "\n",
    "train_df = df[df[time_col_feat].isin(train_steps)].copy()\n",
    "val_df   = df[df[time_col_feat].isin(val_steps)].copy()\n",
    "test_df  = df[df[time_col_feat].isin(test_steps)].copy()\n",
    "\n",
    "print(\"[INFO] Split sizes:\", { \"train\": len(train_df), \"val\": len(val_df), \"test\": len(test_df) })\n",
    "\n",
    "# -----------------------------\n",
    "# Scale features; save preprocessor\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_df[full_features].values)\n",
    "y_train = train_df[\"label\"].values.astype(int)\n",
    "\n",
    "X_val   = scaler.transform(val_df[full_features].values)\n",
    "y_val   = val_df[\"label\"].values.astype(int)\n",
    "\n",
    "X_test  = scaler.transform(test_df[full_features].values)\n",
    "y_test  = test_df[\"label\"].values.astype(int)\n",
    "\n",
    "preproc = {\n",
    "    \"feature_columns\": full_features,\n",
    "    \"scaler\": scaler,\n",
    "    \"label_map\": label_map,\n",
    "    \"time_column\": time_col_feat,\n",
    "    \"txid_column\": tx_col_feat,\n",
    "    \"splits\": {\n",
    "        \"train_steps\": sorted(list(train_steps)),\n",
    "        \"val_steps\":   sorted(list(val_steps)),\n",
    "        \"test_steps\":  sorted(list(test_steps)),\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"preprocessor.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(preproc, f)\n",
    "print(\"[INFO] Saved preprocessor.pkl\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build & train a compact MLP (baseline)\n",
    "# -----------------------------\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "def build_model():\n",
    "    inp = keras.Input(shape=(input_dim,))\n",
    "    x = layers.BatchNormalization()(inp)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inp, out, name=\"elliptic_mlp_baseline\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(name=\"auc\"),\n",
    "            keras.metrics.AUC(name=\"pr_auc\", curve=\"PR\"),\n",
    "            keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Handle imbalance with class weights (robust if a class is missing in train)\n",
    "try:\n",
    "    classes = np.array([0,1])\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\", classes=classes, y=y_train\n",
    "    )\n",
    "    cw = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
    "except Exception:\n",
    "    cw = {0: 1.0, 1: 1.0}\n",
    "print(\"[INFO] Class weights:\", cw)\n",
    "\n",
    "early = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_pr_auc\", mode=\"max\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=40,\n",
    "    batch_size=512,\n",
    "    class_weight=cw,\n",
    "    callbacks=[early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate & choose threshold by F1 on validation\n",
    "# -----------------------------\n",
    "val_prob = model.predict(X_val, batch_size=2048, verbose=0).ravel()\n",
    "test_prob= model.predict(X_test, batch_size=2048, verbose=0).ravel()\n",
    "\n",
    "# Sweep thresholds\n",
    "cand = np.linspace(0.05, 0.95, 181)\n",
    "best_t, best_f1 = 0.5, -1.0\n",
    "for t in cand:\n",
    "    f1 = f1_score(y_val, (val_prob >= t).astype(int), zero_division=0)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_t = float(f1), float(t)\n",
    "print(f\"[INFO] Best threshold on val: t={best_t:.3f} | F1={best_f1:.4f}\")\n",
    "\n",
    "# Final metrics on test\n",
    "auc_roc = roc_auc_score(y_test, test_prob)\n",
    "auc_pr  = average_precision_score(y_test, test_prob)\n",
    "brier   = brier_score_loss(y_test, test_prob)\n",
    "acc05   = accuracy_score(y_test, (test_prob >= 0.5).astype(int))\n",
    "f105    = f1_score(y_test, (test_prob >= 0.5).astype(int), zero_division=0)\n",
    "accT    = accuracy_score(y_test, (test_prob >= best_t).astype(int))\n",
    "f1T     = f1_score(y_test, (test_prob >= best_t).astype(int), zero_division=0)\n",
    "\n",
    "metrics = {\n",
    "    \"n_train\": int(len(X_train)),\n",
    "    \"n_val\":   int(len(X_val)),\n",
    "    \"n_test\":  int(len(X_test)),\n",
    "    \"roc_auc\": float(auc_roc),\n",
    "    \"pr_auc\":  float(auc_pr),\n",
    "    \"brier\":   float(brier),\n",
    "    \"acc_at_0.5\": float(acc05),\n",
    "    \"f1_at_0.5\":  float(f105),\n",
    "    \"acc_at_best_t\": float(accT),\n",
    "    \"f1_at_best_t\":  float(f1T),\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "with open(os.path.join(OUTPUT_DIR, \"threshold.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"best_threshold\": best_t, \"best_f1\": best_f1}, f, indent=2)\n",
    "print(\"[INFO] Saved metrics.json and threshold.json\")\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# -----------------------------\n",
    "# Save model.h5 (as requested)\n",
    "# -----------------------------\n",
    "h5_path = os.path.join(OUTPUT_DIR, \"model.h5\")\n",
    "model.save(h5_path)\n",
    "print(\"[INFO] Saved model.h5 ->\", h5_path)\n",
    "\n",
    "# -----------------------------\n",
    "# Save model_config.yaml\n",
    "# -----------------------------\n",
    "config = {\n",
    "    \"project\": \"GraphGuard — Elliptic Bitcoin Baseline\",\n",
    "    \"paths\": {\n",
    "        \"features_csv\": FEATURES_PATH,\n",
    "        \"classes_csv\":  CLASSES_PATH,\n",
    "        \"edgelist_csv\": EDGES_PATH,\n",
    "        \"output_dir\":   OUTPUT_DIR\n",
    "    },\n",
    "    \"splits\": {\n",
    "        \"train_steps\": sorted(list(train_steps)),\n",
    "        \"val_steps\":   sorted(list(val_steps)),\n",
    "        \"test_steps\":  sorted(list(test_steps))\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"scaler\": \"StandardScaler\",\n",
    "        \"feature_columns\": full_features,\n",
    "        \"extra_graph_features\": [\"in_degree\",\"out_degree\"]\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"type\": \"MLP\",\n",
    "        \"layers\": [256, 128],\n",
    "        \"dropout\": [0.3, 0.2],\n",
    "        \"batch_norm\": True,\n",
    "        \"activation\": \"relu\",\n",
    "        \"loss\": \"binary_crossentropy\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"metrics\": [\"AUC\", \"PR_AUC\", \"BinaryAccuracy\"]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 40,\n",
    "        \"batch_size\": 512,\n",
    "        \"early_stopping\": {\"monitor\": \"val_pr_auc\", \"mode\": \"max\", \"patience\": 5},\n",
    "        \"class_weight\": cw\n",
    "    }\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(OUTPUT_DIR, \"model_config.yaml\")\n",
    "try:\n",
    "    import yaml\n",
    "    with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(config, f, sort_keys=False)\n",
    "except Exception:\n",
    "    # Minimal YAML fallback\n",
    "    def to_yaml(d, indent=0):\n",
    "        lines, pad = [], \"  \" * indent\n",
    "        if isinstance(d, dict):\n",
    "            for k, v in d.items():\n",
    "                if isinstance(v, (dict, list)):\n",
    "                    lines.append(f\"{pad}{k}:\")\n",
    "                    lines.extend(to_yaml(v, indent+1))\n",
    "                else:\n",
    "                    lines.append(f\"{pad}{k}: {repr(v)}\")\n",
    "        elif isinstance(d, list):\n",
    "            for it in d:\n",
    "                if isinstance(it, (dict, list)):\n",
    "                    lines.append(f\"{pad}-\")\n",
    "                    lines.extend(to_yaml(it, indent+1))\n",
    "                else:\n",
    "                    lines.append(f\"{pad}- {repr(it)}\")\n",
    "        return lines\n",
    "    with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(to_yaml(config)))\n",
    "print(\"[INFO] Saved model_config.yaml ->\", yaml_path)\n",
    "\n",
    "print(\"\\n[DONE] Artifacts saved in:\", OUTPUT_DIR)\n",
    "print(\" - preprocessor.pkl\")\n",
    "print(\" - model.h5\")\n",
    "print(\" - model_config.yaml\")\n",
    "print(\" - metrics.json\")\n",
    "print(\" - threshold.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cb5d0-74e7-4ef4-80fd-2e04a5e9a41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
