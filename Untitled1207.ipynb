{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c076acd-27e7-4113-80a4-7c584660d16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved predictions.csv -> C:\\Users\\sagni\\Downloads\\GraphGuard\\predictions.csv\n",
      "[INFO] Saved predictions.jsonl -> C:\\Users\\sagni\\Downloads\\GraphGuard\\predictions.jsonl\n",
      "       txId  timeStep  prob_illicit  pred_label  true_label\n",
      "0  97023208        40      0.987521           1         NaN\n",
      "1  97732542        40      0.981686           1         NaN\n",
      "2  97425476        40      0.999817           1         1.0\n",
      "3  13344762        40      0.994055           1         1.0\n",
      "4  87028631        40      0.967744           1         NaN\n",
      "[INFO] Wrote predict_cli.py -> C:\\Users\\sagni\\Downloads\\GraphGuard\\predict_cli.py\n",
      "\n",
      "Run from terminal, e.g.:\n",
      "  cd \"C:\\Users\\sagni\\Downloads\\GraphGuard\"\n",
      "  python predict_cli.py --mode dataset --subset test\n",
      "  python predict_cli.py --mode txids --txids 230425980,230425981\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GraphGuard â€” Inference & Batch Prediction\n",
    "# - Predicts on TEST window and saves predictions.csv / predictions.jsonl\n",
    "# - Writes predict_cli.py for custom runs (dataset or specific TXIDs)\n",
    "# ============================================================\n",
    "import os, csv, json, pickle, warnings, sys\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional pretty heatmaps if you later want them here too\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    USE_SNS = True\n",
    "except Exception:\n",
    "    USE_SNS = False\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# -----------------------------\n",
    "# Project paths\n",
    "# -----------------------------\n",
    "BASE        = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\"\n",
    "OUTPUT_DIR  = BASE\n",
    "FEATURES_CSV= r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_features.csv\"\n",
    "CLASSES_CSV = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_classes.csv\"\n",
    "EDGES_CSV   = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_edgelist.csv\"\n",
    "\n",
    "PREPROC_PKL = os.path.join(OUTPUT_DIR, \"preprocessor.pkl\")\n",
    "H5_PATH     = os.path.join(OUTPUT_DIR, \"model.h5\")\n",
    "KERAS_PATH  = os.path.join(OUTPUT_DIR, \"model.keras\")  # optional if you saved it\n",
    "THRESH_PATH = os.path.join(OUTPUT_DIR, \"threshold.json\")\n",
    "CFG_YAML    = os.path.join(OUTPUT_DIR, \"model_config.yaml\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Robust CSV reader\n",
    "# -----------------------------\n",
    "def robust_read_csv(path, expected_min_cols=2):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    encodings = [\"utf-8\",\"utf-8-sig\",\"cp1252\",\"latin1\"]\n",
    "    delims    = [\",\",\";\",\"\\t\",\"|\"]\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(8192).decode(\"latin1\", errors=\"ignore\")\n",
    "        sniffed = csv.Sniffer().sniff(head)\n",
    "        if sniffed.delimiter in delims:\n",
    "            delims = [sniffed.delimiter] + [d for d in delims if d != sniffed.delimiter]\n",
    "    except Exception:\n",
    "        pass\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        for sep in delims:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine=\"python\")\n",
    "                if df.shape[1] >= expected_min_cols:\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "    raise RuntimeError(f\"Could not parse {path}. Last error: {last_err}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load artifacts\n",
    "# -----------------------------\n",
    "with open(PREPROC_PKL, \"rb\") as f:\n",
    "    preproc = pickle.load(f)\n",
    "\n",
    "feature_cols = list(preproc[\"feature_columns\"])\n",
    "scaler       = preproc[\"scaler\"]\n",
    "time_col     = preproc[\"time_column\"]\n",
    "txid_col     = preproc[\"txid_column\"]\n",
    "train_steps  = set(preproc[\"splits\"][\"train_steps\"])\n",
    "val_steps    = set(preproc[\"splits\"][\"val_steps\"])\n",
    "test_steps   = set(preproc[\"splits\"][\"test_steps\"])\n",
    "\n",
    "with open(THRESH_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    best_t = float(json.load(f)[\"best_threshold\"])\n",
    "\n",
    "# If you have a YAML config, prefer paths from there (optional)\n",
    "try:\n",
    "    import yaml\n",
    "    if os.path.exists(CFG_YAML):\n",
    "        with open(CFG_YAML, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        FEATURES_CSV = cfg[\"paths\"].get(\"features_csv\", FEATURES_CSV)\n",
    "        CLASSES_CSV  = cfg[\"paths\"].get(\"classes_csv\",  CLASSES_CSV)\n",
    "        EDGES_CSV    = cfg[\"paths\"].get(\"edgelist_csv\", EDGES_CSV)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------------\n",
    "# Load data and prep features\n",
    "# -----------------------------\n",
    "df_feat = robust_read_csv(FEATURES_CSV, expected_min_cols=3)\n",
    "df_cls  = robust_read_csv(CLASSES_CSV,  expected_min_cols=2)\n",
    "df_edge = robust_read_csv(EDGES_CSV,    expected_min_cols=2)\n",
    "\n",
    "# Column names as saved during training\n",
    "feat_cols = list(df_feat.columns)\n",
    "tx_col_feat   = feat_cols[0]\n",
    "time_col_feat = feat_cols[1]\n",
    "assert tx_col_feat == txid_col, f\"TX ID column mismatch: {tx_col_feat} vs {txid_col}\"\n",
    "assert time_col_feat == time_col, f\"Time column mismatch: {time_col_feat} vs {time_col}\"\n",
    "\n",
    "cls_cols  = list(df_cls.columns)\n",
    "tx_col_cls = cls_cols[0]\n",
    "class_col  = cls_cols[1]\n",
    "\n",
    "edge_cols = list(df_edge.columns)\n",
    "src_col, dst_col = edge_cols[0], edge_cols[1]\n",
    "\n",
    "# Force string TX IDs everywhere\n",
    "df_feat[tx_col_feat] = df_feat[tx_col_feat].astype(str)\n",
    "df_cls[tx_col_cls]   = df_cls[tx_col_cls].astype(str)\n",
    "df_edge[src_col]     = df_edge[src_col].astype(str)\n",
    "df_edge[dst_col]     = df_edge[dst_col].astype(str)\n",
    "\n",
    "# Map labels (drop unknowns)\n",
    "df_cls[class_col] = df_cls[class_col].astype(str).str.lower().str.strip()\n",
    "label_map = {\"1\":0, \"2\":1, \"licit\":0, \"illicit\":1}\n",
    "df_cls[\"label\"] = df_cls[class_col].map(label_map)\n",
    "df_cls = df_cls[~df_cls[\"label\"].isna()].copy()\n",
    "df_cls[\"label\"] = df_cls[\"label\"].astype(int)\n",
    "\n",
    "# Compute degrees\n",
    "in_deg  = df_edge.groupby(dst_col).size().rename(\"in_degree\")\n",
    "out_deg = df_edge.groupby(src_col).size().rename(\"out_degree\")\n",
    "deg_df  = pd.concat([in_deg, out_deg], axis=1).fillna(0.0).reset_index()\n",
    "deg_df.rename(columns={deg_df.columns[0]: tx_col_feat}, inplace=True)\n",
    "\n",
    "# Merge & clean\n",
    "df_feat[time_col_feat] = pd.to_numeric(df_feat[time_col_feat], errors=\"coerce\")\n",
    "df = df_feat.merge(deg_df, on=tx_col_feat, how=\"left\")\n",
    "df[[\"in_degree\",\"out_degree\"]] = df[[\"in_degree\",\"out_degree\"]].fillna(0.0)\n",
    "df = df.merge(df_cls[[tx_col_cls,\"label\"]], left_on=tx_col_feat, right_on=tx_col_cls, how=\"left\")\n",
    "if tx_col_cls in df.columns and tx_col_cls != tx_col_feat:\n",
    "    df = df.drop(columns=[tx_col_cls])\n",
    "\n",
    "# Keep only columns needed for features (drop NA rows)\n",
    "for c in feature_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=feature_cols + [time_col_feat]).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Load model\n",
    "# -----------------------------\n",
    "model = None\n",
    "if os.path.exists(KERAS_PATH):\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(KERAS_PATH, safe_mode=False)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not load model.keras:\", e)\n",
    "\n",
    "if model is None and os.path.exists(H5_PATH):\n",
    "    model = tf.keras.models.load_model(H5_PATH)\n",
    "\n",
    "if model is None:\n",
    "    raise FileNotFoundError(\"No model found (model.keras / model.h5). Train first.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: predict on a dataframe slice\n",
    "# -----------------------------\n",
    "def predict_df(df_slice: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = scaler.transform(df_slice[feature_cols].values)\n",
    "    prob = model.predict(X, batch_size=4096, verbose=0).ravel()\n",
    "    pred = (prob >= best_t).astype(int)\n",
    "    out = pd.DataFrame({\n",
    "        \"txId\": df_slice[txid_col].values,\n",
    "        \"timeStep\": df_slice[time_col].values,\n",
    "        \"prob_illicit\": prob,\n",
    "        \"pred_label\": pred\n",
    "    })\n",
    "    if \"label\" in df_slice.columns:\n",
    "        out[\"true_label\"] = df_slice[\"label\"].values\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Prediction on TEST window & save\n",
    "# -----------------------------\n",
    "df_test = df[df[time_col].isin(test_steps)].copy()\n",
    "pred_test = predict_df(df_test)\n",
    "\n",
    "pred_csv  = os.path.join(OUTPUT_DIR, \"predictions.csv\")\n",
    "pred_json = os.path.join(OUTPUT_DIR, \"predictions.jsonl\")\n",
    "\n",
    "pred_test.to_csv(pred_csv, index=False)\n",
    "with open(pred_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in pred_test.iterrows():\n",
    "        rec = row.to_dict()\n",
    "        rec[\"pred_label_name\"] = \"illicit\" if rec[\"pred_label\"]==1 else \"licit\"\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"[INFO] Saved predictions.csv -> {pred_csv}\")\n",
    "print(f\"[INFO] Saved predictions.jsonl -> {pred_json}\")\n",
    "print(pred_test.head())\n",
    "\n",
    "# ============================================================\n",
    "# (Optional) Write a CLI script for arbitrary prediction runs\n",
    "#   Usage examples:\n",
    "#     python predict_cli.py --mode dataset\n",
    "#     python predict_cli.py --mode dataset --subset all\n",
    "#     python predict_cli.py --mode txids --txids 1000000,1000001\n",
    "# ============================================================\n",
    "cli_code = r'''#!/usr/bin/env python\n",
    "import os, csv, json, pickle, argparse, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def robust_read_csv(path, expected_min_cols=2):\n",
    "    encodings = [\"utf-8\",\"utf-8-sig\",\"cp1252\",\"latin1\"]\n",
    "    delims    = [\",\",\";\",\"\\t\",\"|\"]\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(8192).decode(\"latin1\", errors=\"ignore\")\n",
    "        sniffed = csv.Sniffer().sniff(head)\n",
    "        if sniffed.delimiter in delims:\n",
    "            delims = [sniffed.delimiter] + [d for d in delims if d != sniffed.delimiter]\n",
    "    except Exception:\n",
    "        pass\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        for sep in delims:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine=\"python\")\n",
    "                if df.shape[1] >= expected_min_cols:\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "    raise RuntimeError(f\"Could not parse {path}. Last error: {last_err}\")\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"GraphGuard prediction CLI (Elliptic)\")\n",
    "    p.add_argument(\"--base\", default=r\"C:\\Users\\sagni\\Downloads\\GraphGuard\")\n",
    "    p.add_argument(\"--features_csv\", default=r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_features.csv\")\n",
    "    p.add_argument(\"--classes_csv\",  default=r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_classes.csv\")\n",
    "    p.add_argument(\"--edges_csv\",    default=r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_edgelist.csv\")\n",
    "    p.add_argument(\"--mode\", choices=[\"dataset\",\"txids\"], default=\"dataset\")\n",
    "    p.add_argument(\"--subset\", choices=[\"train\",\"val\",\"test\",\"all\"], default=\"test\")\n",
    "    p.add_argument(\"--txids\", help=\"Comma-separated txIds (only for --mode txids)\")\n",
    "    args = p.parse_args()\n",
    "\n",
    "    BASE = args.base\n",
    "    PREPROC_PKL = os.path.join(BASE, \"preprocessor.pkl\")\n",
    "    H5_PATH     = os.path.join(BASE, \"model.h5\")\n",
    "    KERAS_PATH  = os.path.join(BASE, \"model.keras\")\n",
    "    THRESH_PATH = os.path.join(BASE, \"threshold.json\")\n",
    "\n",
    "    with open(PREPROC_PKL, \"rb\") as f:\n",
    "        preproc = pickle.load(f)\n",
    "    feature_cols = list(preproc[\"feature_columns\"])\n",
    "    scaler       = preproc[\"scaler\"]\n",
    "    time_col     = preproc[\"time_column\"]\n",
    "    txid_col     = preproc[\"txid_column\"]\n",
    "    train_steps  = set(preproc[\"splits\"][\"train_steps\"])\n",
    "    val_steps    = set(preproc[\"splits\"][\"val_steps\"])\n",
    "    test_steps   = set(preproc[\"splits\"][\"test_steps\"])\n",
    "\n",
    "    with open(THRESH_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        best_t = float(json.load(f)[\"best_threshold\"])\n",
    "\n",
    "    # Load data\n",
    "    df_feat = robust_read_csv(args.features_csv, expected_min_cols=3)\n",
    "    df_cls  = robust_read_csv(args.classes_csv,  expected_min_cols=2)\n",
    "    df_edge = robust_read_csv(args.edges_csv,    expected_min_cols=2)\n",
    "\n",
    "    feat_cols = list(df_feat.columns)\n",
    "    tx_col_feat   = feat_cols[0]\n",
    "    time_col_feat = feat_cols[1]\n",
    "    assert tx_col_feat == txid_col, f\"TX ID column mismatch: {tx_col_feat} vs {txid_col}\"\n",
    "    assert time_col_feat == time_col, f\"Time column mismatch: {time_col_feat} vs {time_col}\"\n",
    "\n",
    "    cls_cols  = list(df_cls.columns)\n",
    "    tx_col_cls = cls_cols[0]\n",
    "    class_col  = cls_cols[1]\n",
    "\n",
    "    edge_cols = list(df_edge.columns)\n",
    "    src_col, dst_col = edge_cols[0], edge_cols[1]\n",
    "\n",
    "    # Force TXID to string\n",
    "    df_feat[tx_col_feat] = df_feat[tx_col_feat].astype(str)\n",
    "    df_cls[tx_col_cls]   = df_cls[tx_col_cls].astype(str)\n",
    "    df_edge[src_col]     = df_edge[src_col].astype(str)\n",
    "    df_edge[dst_col]     = df_edge[dst_col].astype(str)\n",
    "\n",
    "    # Labels (optional)\n",
    "    df_cls[class_col] = df_cls[class_col].astype(str).str.lower().str.strip()\n",
    "    label_map = {\"1\":0, \"2\":1, \"licit\":0, \"illicit\":1}\n",
    "    df_cls[\"label\"] = df_cls[class_col].map(label_map)\n",
    "    df_cls[\"label\"] = df_cls[\"label\"].astype(\"Int64\")  # allow NA\n",
    "\n",
    "    # Degrees\n",
    "    in_deg  = df_edge.groupby(dst_col).size().rename(\"in_degree\")\n",
    "    out_deg = df_edge.groupby(src_col).size().rename(\"out_degree\")\n",
    "    deg_df  = pd.concat([in_deg, out_deg], axis=1).fillna(0.0).reset_index()\n",
    "    deg_df.rename(columns={deg_df.columns[0]: tx_col_feat}, inplace=True)\n",
    "\n",
    "    # Merge\n",
    "    df_feat[time_col_feat] = pd.to_numeric(df_feat[time_col_feat], errors=\"coerce\")\n",
    "    df = df_feat.merge(deg_df, on=tx_col_feat, how=\"left\")\n",
    "    df[[\"in_degree\",\"out_degree\"]] = df[[\"in_degree\",\"out_degree\"]].fillna(0.0)\n",
    "    df = df.merge(df_cls[[tx_col_cls,\"label\"]], left_on=tx_col_feat, right_on=tx_col_cls, how=\"left\")\n",
    "    if tx_col_cls in df.columns and tx_col_cls != tx_col_feat:\n",
    "        df = df.drop(columns=[tx_col_cls])\n",
    "\n",
    "    # Keep numeric features\n",
    "    for c in feature_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=feature_cols + [time_col_feat]).reset_index(drop=True)\n",
    "\n",
    "    # Subset\n",
    "    if args.mode == \"dataset\":\n",
    "        if args.subset == \"train\":\n",
    "            mask = df[time_col].isin(train_steps)\n",
    "        elif args.subset == \"val\":\n",
    "            mask = df[time_col].isin(val_steps)\n",
    "        elif args.subset == \"test\":\n",
    "            mask = df[time_col].isin(test_steps)\n",
    "        else:\n",
    "            mask = np.ones(len(df), dtype=bool)\n",
    "        df_slice = df[mask].copy()\n",
    "    else:\n",
    "        # txids mode\n",
    "        if not args.txids:\n",
    "            raise SystemExit(\"--txids is required for --mode txids\")\n",
    "        want = set([s.strip() for s in args.txids.split(\",\") if s.strip()])\n",
    "        df_slice = df[df[txid_col].isin(want)].copy()\n",
    "        if df_slice.empty:\n",
    "            raise SystemExit(\"None of the requested txIds were found in features CSV.\")\n",
    "\n",
    "    # Load model\n",
    "    model = None\n",
    "    if os.path.exists(KERAS_PATH):\n",
    "        try:\n",
    "            model = tf.keras.models.load_model(KERAS_PATH, safe_mode=False)\n",
    "        except Exception:\n",
    "            model = None\n",
    "    if model is None and os.path.exists(H5_PATH):\n",
    "        model = tf.keras.models.load_model(H5_PATH)\n",
    "    if model is None:\n",
    "        raise SystemExit(\"No model found (model.keras / model.h5).\")\n",
    "\n",
    "    # Predict\n",
    "    X = scaler.transform(df_slice[feature_cols].values)\n",
    "    prob = model.predict(X, batch_size=4096, verbose=0).ravel()\n",
    "    pred = (prob >= best_t).astype(int)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"txId\": df_slice[txid_col].values,\n",
    "        \"timeStep\": df_slice[time_col].values,\n",
    "        \"prob_illicit\": prob,\n",
    "        \"pred_label\": pred\n",
    "    })\n",
    "    if \"label\" in df_slice.columns:\n",
    "        out[\"true_label\"] = df_slice[\"label\"].astype(\"Int64\").values\n",
    "\n",
    "    # Save\n",
    "    name = f\"predictions_{args.mode}_{args.subset}.csv\" if args.mode==\"dataset\" else \"predictions_txids.csv\"\n",
    "    csv_path = os.path.join(BASE, name)\n",
    "    out.to_csv(csv_path, index=False)\n",
    "    jsonl_path = csv_path.replace(\".csv\",\".jsonl\")\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in out.iterrows():\n",
    "            rec = row.to_dict()\n",
    "            rec[\"pred_label_name\"] = \"illicit\" if rec[\"pred_label\"]==1 else \"licit\"\n",
    "            f.write(json.dumps(rec)+\"\\n\")\n",
    "\n",
    "    print(f\"[OK] Wrote: {csv_path}\")\n",
    "    print(f\"[OK] Wrote: {jsonl_path}\")\n",
    "    print(out.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "# write CLI\n",
    "with open(os.path.join(OUTPUT_DIR, \"predict_cli.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cli_code)\n",
    "\n",
    "print(\"[INFO] Wrote predict_cli.py ->\", os.path.join(OUTPUT_DIR, \"predict_cli.py\"))\n",
    "print(\"\\nRun from terminal, e.g.:\")\n",
    "print(r'  cd \"C:\\Users\\sagni\\Downloads\\GraphGuard\"')\n",
    "print(r'  python predict_cli.py --mode dataset --subset test')\n",
    "print(r'  python predict_cli.py --mode txids --txids 230425980,230425981')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd337410-c7dd-41d7-a8d2-8eaca7061ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
