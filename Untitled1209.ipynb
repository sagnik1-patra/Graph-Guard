{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596aa7fd-8676-46a9-9181-964cdfb4bd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['C:\\\\Users\\\\sagni']\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [4508] using StatReload\n"
     ]
    }
   ],
   "source": [
    "import os, io, csv, json, pickle, base64, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, Any, List\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import HTMLResponse\n",
    "from starlette.staticfiles import StaticFiles\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "BASE        = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\"\n",
    "FEATURES_CSV= r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_features.csv\"\n",
    "CLASSES_CSV = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_classes.csv\"\n",
    "EDGES_CSV   = r\"C:\\Users\\sagni\\Downloads\\GraphGuard\\archive (1)\\elliptic_bitcoin_dataset\\elliptic_txs_edgelist.csv\"\n",
    "\n",
    "PREPROC_PKL = os.path.join(BASE, \"preprocessor.pkl\")\n",
    "H5_PATH     = os.path.join(BASE, \"model.h5\")\n",
    "KERAS_PATH  = os.path.join(BASE, \"model.keras\")  # optional\n",
    "THRESH_PATH = os.path.join(BASE, \"threshold.json\")\n",
    "\n",
    "# Globals (lazy init)\n",
    "_preproc = None\n",
    "_model   = None\n",
    "_best_t  = None\n",
    "_df      = None\n",
    "_df_idx  = None\n",
    "_G       = None\n",
    "_feature_cols = None\n",
    "_time_col    = None\n",
    "_txid_col    = None\n",
    "\n",
    "def robust_read_csv(path, expected_min_cols=2):\n",
    "    delims = [\",\",\";\",\"\\t\",\"|\"]\n",
    "    encs   = [\"utf-8\",\"utf-8-sig\",\"cp1252\",\"latin1\"]\n",
    "    last_err=None\n",
    "    for enc in encs:\n",
    "        for sep in delims:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine=\"python\")\n",
    "                if df.shape[1] >= expected_min_cols:\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err=e\n",
    "    raise RuntimeError(f\"Could not parse {path}. Last error: {last_err}\")\n",
    "\n",
    "def _lazy_load_preproc():\n",
    "    global _preproc, _feature_cols, _time_col, _txid_col, _best_t\n",
    "    if _preproc is None:\n",
    "        if not os.path.exists(PREPROC_PKL):\n",
    "            raise RuntimeError(f\"Missing preprocessor.pkl at {PREPROC_PKL}\")\n",
    "        with open(PREPROC_PKL, \"rb\") as f:\n",
    "            _preproc = pickle.load(f)\n",
    "        _feature_cols = list(_preproc[\"feature_columns\"])\n",
    "        _time_col     = _preproc[\"time_column\"]\n",
    "        _txid_col     = _preproc[\"txid_column\"]\n",
    "        if not os.path.exists(THRESH_PATH):\n",
    "            raise RuntimeError(f\"Missing threshold.json at {THRESH_PATH}\")\n",
    "        with open(THRESH_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            _best_t = float(json.load(f)[\"best_threshold\"])\n",
    "    return _preproc\n",
    "\n",
    "def _lazy_load_model():\n",
    "    global _model\n",
    "    if _model is not None:\n",
    "        return _model\n",
    "    if os.path.exists(KERAS_PATH):\n",
    "        try:\n",
    "            _model = tf.keras.models.load_model(KERAS_PATH, safe_mode=False)\n",
    "            return _model\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] model.keras load failed, falling back to H5:\", e)\n",
    "    if os.path.exists(H5_PATH):\n",
    "        _model = tf.keras.models.load_model(H5_PATH)\n",
    "        return _model\n",
    "    raise RuntimeError(f\"Model not found: {KERAS_PATH} or {H5_PATH}\")\n",
    "\n",
    "def _lazy_load_data_graph():\n",
    "    global _df, _df_idx, _G\n",
    "    if _df is not None and _df_idx is not None and _G is not None:\n",
    "        return _df, _df_idx, _G\n",
    "\n",
    "    pre = _lazy_load_preproc()\n",
    "    feature_cols = _feature_cols\n",
    "    time_col     = _time_col\n",
    "    txid_col     = _txid_col\n",
    "\n",
    "    df_feat = robust_read_csv(FEATURES_CSV, expected_min_cols=3)\n",
    "    df_cls  = robust_read_csv(CLASSES_CSV,  expected_min_cols=2)\n",
    "    df_edge = robust_read_csv(EDGES_CSV,    expected_min_cols=2)\n",
    "\n",
    "    feat_cols = list(df_feat.columns)\n",
    "    tx_col_feat, time_col_feat = feat_cols[0], feat_cols[1]\n",
    "    cls_cols = list(df_cls.columns)\n",
    "    tx_col_cls, class_col = cls_cols[0], cls_cols[1]\n",
    "    edge_cols = list(df_edge.columns)\n",
    "    src_col, dst_col = edge_cols[0], edge_cols[1]\n",
    "\n",
    "    # IDs as string\n",
    "    df_feat[tx_col_feat] = df_feat[tx_col_feat].astype(str)\n",
    "    df_cls[tx_col_cls]   = df_cls[tx_col_cls].astype(str)\n",
    "    df_edge[src_col]     = df_edge[src_col].astype(str)\n",
    "    df_edge[dst_col]     = df_edge[dst_col].astype(str)\n",
    "\n",
    "    # Degrees\n",
    "    in_deg  = df_edge.groupby(dst_col).size().rename(\"in_degree\")\n",
    "    out_deg = df_edge.groupby(src_col).size().rename(\"out_degree\")\n",
    "    deg_df  = pd.concat([in_deg, out_deg], axis=1).fillna(0.0).reset_index()\n",
    "    deg_df.rename(columns={deg_df.columns[0]: tx_col_feat}, inplace=True)\n",
    "\n",
    "    # Merge\n",
    "    df_feat[time_col_feat] = pd.to_numeric(df_feat[time_col_feat], errors=\"coerce\")\n",
    "    df = df_feat.merge(deg_df, on=tx_col_feat, how=\"left\")\n",
    "    df[[\"in_degree\",\"out_degree\"]] = df[[\"in_degree\",\"out_degree\"]].fillna(0.0)\n",
    "\n",
    "    # Optional labels\n",
    "    df_cls[class_col] = df_cls[class_col].astype(str).str.lower().str.strip()\n",
    "    df_cls[\"label\"] = df_cls[class_col].map({\"1\":0,\"2\":1,\"licit\":0,\"illicit\":1}).astype(\"Int64\")\n",
    "    df = df.merge(df_cls[[tx_col_cls,\"label\"]], left_on=tx_col_feat, right_on=tx_col_cls, how=\"left\")\n",
    "    if tx_col_cls in df.columns and tx_col_cls != tx_col_feat:\n",
    "        df = df.drop(columns=[tx_col_cls])\n",
    "\n",
    "    # Keep numeric features\n",
    "    for c in feature_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[time_col_feat] + feature_cols).reset_index(drop=True)\n",
    "\n",
    "    # Build undirected graph for ego subgraph\n",
    "    G = nx.from_pandas_edgelist(df_edge, source=src_col, target=dst_col, create_using=nx.Graph())\n",
    "\n",
    "    _df, _df_idx, _G = df, df.set_index(tx_col_feat), G\n",
    "    # align names\n",
    "    assert time_col == time_col_feat and txid_col == tx_col_feat, \"Saved columns don't match CSV headers.\"\n",
    "    return _df, _df_idx, _G\n",
    "\n",
    "def _scale_and_predict(x_row: np.ndarray) -> (float, int):\n",
    "    pre = _lazy_load_preproc()\n",
    "    model = _lazy_load_model()\n",
    "    scaler = pre[\"scaler\"]\n",
    "    Xs = scaler.transform(x_row)\n",
    "    prob = float(model.predict(Xs, verbose=0).ravel()[0])\n",
    "    pred = 1 if prob >= _best_t else 0\n",
    "    return prob, pred\n",
    "\n",
    "def _grad_input_importance(x_scaled: np.ndarray) -> np.ndarray:\n",
    "    x = tf.convert_to_tensor(x_scaled.astype(\"float32\"))\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        p = _lazy_load_model()(x, training=False)\n",
    "        y = p[:, 0]\n",
    "    grads = tape.gradient(y, x).numpy()[0]\n",
    "    contrib = np.abs(grads * x.numpy()[0])\n",
    "    return contrib\n",
    "\n",
    "def _get_tx_features(txid: str) -> Dict[str, Any]:\n",
    "    df, df_idx, _ = _lazy_load_data_graph()\n",
    "    pre = _lazy_load_preproc()\n",
    "    feature_cols = _feature_cols\n",
    "    time_col     = _time_col\n",
    "    if txid not in df_idx.index:\n",
    "        raise KeyError(f\"txId '{txid}' not found.\")\n",
    "    row = df_idx.loc[txid]\n",
    "    x = row[feature_cols].values.reshape(1, -1)\n",
    "    t = int(row[time_col])\n",
    "    label = None\n",
    "    if \"label\" in df_idx.columns and not pd.isna(row.get(\"label\", pd.NA)):\n",
    "        label = int(row[\"label\"])\n",
    "    return {\"x\": x, \"time\": t, \"label\": label}\n",
    "\n",
    "def _ego_subgraph_png(txid: str, k: int = 2, max_nodes: int = 150):\n",
    "    _, _, G = _lazy_load_data_graph()\n",
    "    if txid not in G:\n",
    "        raise KeyError(f\"txId '{txid}' has no edges.\")\n",
    "    nodes = list(nx.ego_graph(G, txid, radius=k).nodes())\n",
    "    if len(nodes) > max_nodes:\n",
    "        nodes = nodes[:max_nodes]\n",
    "    SG = G.subgraph(nodes).copy()\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    pos = nx.spring_layout(SG, seed=42, k=1/np.sqrt(max(len(SG),1)))\n",
    "    node_colors = [\"red\" if n == txid else \"steelblue\" for n in SG.nodes()]\n",
    "    nx.draw_networkx_nodes(SG, pos, node_color=node_colors, node_size=80, alpha=0.9, linewidths=0.3, edgecolors=\"white\")\n",
    "    nx.draw_networkx_edges(SG, pos, alpha=0.4, width=0.8)\n",
    "    nx.draw_networkx_labels(SG, pos, labels={txid: txid}, font_size=8)\n",
    "    buf = io.BytesIO()\n",
    "    plt.tight_layout(); plt.savefig(buf, format=\"png\", dpi=150); plt.close()\n",
    "    return {\"nodes\":[{\"id\":n} for n in SG.nodes()], \"edges\":[{\"source\":u,\"target\":v} for u,v in SG.edges()]}, base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# --------------------------\n",
    "# FastAPI app\n",
    "# --------------------------\n",
    "app = FastAPI(title=\"GraphGuard API\", version=\"1.0.1\", description=\"Fraud scoring + ego subgraph (lazy-load)\")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"], allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"],\n",
    ")\n",
    "app.mount(\"/static\", StaticFiles(directory=BASE), name=\"static\")\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "def root():\n",
    "    return HTMLResponse('<meta http-equiv=\"refresh\" content=\"0; url=/static/index.html\">')\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    try:\n",
    "        _lazy_load_preproc()\n",
    "        return {\"status\":\"ok\",\"threshold\":_best_t,\"feature_dim\":len(_feature_cols)}\n",
    "    except Exception as e:\n",
    "        return {\"status\":\"error\",\"detail\":str(e)}\n",
    "\n",
    "@app.get(\"/selftest\")\n",
    "def selftest():\n",
    "    try:\n",
    "        df, df_idx, G = _lazy_load_data_graph()\n",
    "        any_tx = str(df_idx.index[0])\n",
    "        info = _get_tx_features(any_tx)\n",
    "        prob, pred = _scale_and_predict(info[\"x\"])\n",
    "        return {\"status\":\"ok\",\"example_txId\":any_tx,\"prob_illicit\":prob,\"pred_label\":pred,\"n_nodes\":int(G.number_of_nodes()),\"n_edges\":int(G.number_of_edges())}\n",
    "    except Exception as e:\n",
    "        return {\"status\":\"error\",\"detail\":str(e)}\n",
    "\n",
    "@app.post(\"/score\")\n",
    "async def score(payload: Dict[str, Any]):\n",
    "    mode = payload.get(\"mode\",\"txid\")\n",
    "    try:\n",
    "        _lazy_load_preproc(); _lazy_load_model(); _lazy_load_data_graph()\n",
    "        if mode == \"txid\":\n",
    "            txid = str(payload.get(\"txId\",\"\")).strip()\n",
    "            if not txid:\n",
    "                raise ValueError(\"txId required.\")\n",
    "            info = _get_tx_features(txid)\n",
    "            prob, pred = _scale_and_predict(info[\"x\"])\n",
    "            x_scaled = _preproc[\"scaler\"].transform(info[\"x\"])\n",
    "            contrib  = _grad_input_importance(x_scaled)\n",
    "            top_idx  = np.argsort(contrib)[::-1][:10]\n",
    "            top_feats= [{\"feature\": _feature_cols[i], \"score\": float(contrib[i])} for i in top_idx]\n",
    "            return {\"txId\":txid,\"timeStep\":info[\"time\"],\"prob_illicit\":prob,\"pred_label\":pred,\"true_label\":info[\"label\"],\"threshold\":_best_t,\"top_features\":top_feats}\n",
    "        elif mode == \"payload\":\n",
    "            feats = payload.get(\"features\",{})\n",
    "            missing = [c for c in _feature_cols if c not in feats]\n",
    "            if missing: raise ValueError(f\"Missing features: {missing[:10]}...\")\n",
    "            x = np.array([[float(feats[c]) for c in _feature_cols]], dtype=\"float32\")\n",
    "            prob, pred = _scale_and_predict(x)\n",
    "            x_scaled = _preproc[\"scaler\"].transform(x)\n",
    "            contrib  = _grad_input_importance(x_scaled)\n",
    "            top_idx  = np.argsort(contrib)[::-1][:10]\n",
    "            top_feats= [{\"feature\": _feature_cols[i], \"score\": float(contrib[i])} for i in top_idx]\n",
    "            return {\"prob_illicit\":prob,\"pred_label\":pred,\"threshold\":_best_t,\"top_features\":top_feats}\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'txid' or 'payload'\")\n",
    "    except KeyError as e:\n",
    "        raise HTTPException(status_code=404, detail=str(e))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "@app.post(\"/explain\")\n",
    "async def explain(payload: Dict[str, Any]):\n",
    "    txid = str(payload.get(\"txId\",\"\")).strip()\n",
    "    k = int(payload.get(\"k\",2)); max_nodes = int(payload.get(\"max_nodes\",150))\n",
    "    if not txid:\n",
    "        raise HTTPException(status_code=400, detail=\"txId required\")\n",
    "    try:\n",
    "        _lazy_load_preproc(); _lazy_load_model(); _lazy_load_data_graph()\n",
    "        info = _get_tx_features(txid)\n",
    "        prob, pred = _scale_and_predict(info[\"x\"])\n",
    "        x_scaled = _preproc[\"scaler\"].transform(info[\"x\"])\n",
    "        contrib  = _grad_input_importance(x_scaled)\n",
    "        top_idx  = np.argsort(contrib)[::-1][:10]\n",
    "        top_feats= [{\"feature\": _feature_cols[i], \"score\": float(contrib[i])} for i in top_idx]\n",
    "        subgraph_json, b64 = _ego_subgraph_png(txid, k=k, max_nodes=max_nodes)\n",
    "        return {\"txId\":txid,\"timeStep\":info[\"time\"],\"prob_illicit\":prob,\"pred_label\":pred,\"true_label\":info[\"label\"],\"threshold\":_best_t,\"top_features\":top_feats,\"subgraph\":subgraph_json,\"subgraph_png_base64\":b64}\n",
    "    except KeyError as e:\n",
    "        raise HTTPException(status_code=404, detail=str(e))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(\"app:app\", host=\"127.0.0.1\", port=8000, reload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d21e5-e674-4803-954f-5bd8d5c7af29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
